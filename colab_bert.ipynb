{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CabjYMte_tYT",
        "QJVb_25hg0GQ",
        "Zszni5fThq_n",
        "7u6RfEcshrmy"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMX2xowmt/ymTCXft0TAQjM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codylw2/CourseProject/blob/main/colab_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CabjYMte_tYT"
      },
      "source": [
        "# General Setup\r\n",
        "This notebook should be ran using a GPU and high memory. This requires Colab Pro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1voer_oVSL0"
      },
      "source": [
        "This initial code block is where I ensure that all of the required modules are installed for this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1zcht0Hgwy3"
      },
      "source": [
        "!pip install -q PyDrive\r\n",
        "!pip install -q metapy\r\n",
        "!pip install -q pytoml\r\n",
        "!pip install -q tensorflow_ranking"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJVb_25hg0GQ"
      },
      "source": [
        "# Authenticate for Google Drive\r\n",
        "There is a file size limitation for my github repository so I have added the files that will be used for this session into my Google Drive and made them publicly shareable. In order to access them you must authenticate to Google. It should not matter what account you use to do this since the files are public."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3tulhWWg7Rj"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\r\n",
        "from pydrive.drive import GoogleDrive\r\n",
        "from google.colab import auth\r\n",
        "from oauth2client.client import GoogleCredentials\r\n",
        "auth.authenticate_user()\r\n",
        "gauth = GoogleAuth()\r\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\r\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eWWGDm8g9kc"
      },
      "source": [
        "# Acquire Files\r\n",
        "This section concerns itself with acquiring the files that are required to run the later scripts. It will also download a tuned version of the model so that there is no reason to run training unless desired. All of the scripts are stored within the git repo and that is the first thing download. The remaining supporting files are stored in my Google Drive account due to filesize limitations on github. The files are publicly available so anyone should be able to download and use them. The Google Drive files are downloaded through the PyDrive python module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysH4IQV6hCVS"
      },
      "source": [
        "!git clone https://github.com/codylw2/CourseProject.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfI-gIsUVeXt"
      },
      "source": [
        "This code block is where a selection of variables that will be used throughout this notebook are defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rivw10BhC_o"
      },
      "source": [
        "%cd /content/CourseProject/competition/bert\r\n",
        "\r\n",
        "WORKDIR = !pwd\r\n",
        "WORKDIR = WORKDIR[0]\r\n",
        "BASE = WORKDIR + \"/../..\"\r\n",
        "MODEL_DIR = WORKDIR + \"/checkpoints/uncased_L-4_H-256_A-4_TF2\"\r\n",
        "DATASET_DIR = BASE + \"/competition/datasets\"\r\n",
        "JSON_DIR = BASE + \"/competition/json_data\"\r\n",
        "TUNED_MODEL_DIR = WORKDIR + \"/bert_finetuned\"\r\n",
        "VOCAB_FILE = DATASET_DIR + \"/bert_vocab.txt\"\r\n",
        "\r\n",
        "!export CUDA_VISIBLE_DEVICES=0\r\n",
        "QUERY_TOKENS = \"question_tokens\"\r\n",
        "SEQ_LENGTH = 512\r\n",
        "LIST_SIZE = 70"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RDuIIZ0RQHz"
      },
      "source": [
        "The next code block contains the definition of the function that is used to download folders from Google Drive. It iterates over all of the objects in the folder that it is called on. If the object is a file then it will download it but if it is another folder then it will recurse over it as well until everything has been downloaded.\r\n",
        "\r\n",
        "Initial source for how to download a folder: https://stackoverflow.com/questions/47002558/downloading-all-of-the-files-in-a-specific-folder-with-pydrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dqeAu0r5f9k"
      },
      "source": [
        "import os\r\n",
        "def download_folder(dir_name, dir_id):\r\n",
        "  curr_wrkdir = os.getcwd()\r\n",
        "  if not os.path.exists(dir_name):\r\n",
        "    os.makedirs(dir_name)\r\n",
        "  os.chdir(dir_name)\r\n",
        "\r\n",
        "  file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(dir_id)}).GetList()\r\n",
        "  for i, file1 in enumerate(sorted(file_list, key = lambda x: x['title']), start=1):\r\n",
        "    print('Downloading from GDrive ({}/{}): {} '.format(i, len(file_list), os.path.join(dir_name, file1['title'])))\r\n",
        "    try:\r\n",
        "      file1.GetContentFile(file1['title'])\r\n",
        "    except:\r\n",
        "      download_folder(os.path.join(dir_name, file1['title']), file1['id'])\r\n",
        "\r\n",
        "  os.chdir(curr_wrkdir)\r\n",
        "  return"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xw3CAf4Seog"
      },
      "source": [
        "The links shown below are what is generated by Google Driver when you get the link for a file. Contained within the link is an 'id' for the file/folder that you can use to download it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoiQoPkuBkUy"
      },
      "source": [
        "# https://drive.google.com/drive/folders/1vN0aCZBgjK3lDng5z0UGXt0iA8a2neUV?usp=sharing\r\n",
        "folder_id = \"1vN0aCZBgjK3lDng5z0UGXt0iA8a2neUV\"\r\n",
        "download_folder(JSON_DIR, folder_id)\r\n",
        "\r\n",
        "# https://drive.google.com/drive/folders/1yPW8MJDugNBrBKV7zPrJeyD8Zb53wrl4?usp=sharing\r\n",
        "folder_id = \"1yPW8MJDugNBrBKV7zPrJeyD8Zb53wrl4\"\r\n",
        "download_folder(TUNED_MODEL_DIR, folder_id)\r\n",
        "\r\n",
        "# https://drive.google.com/drive/folders/1sIpXa9g4zE0S-nBogdlDc-wDOtBPxt3-?usp=sharing\r\n",
        "folder_id = \"1sIpXa9g4zE0S-nBogdlDc-wDOtBPxt3-\"\r\n",
        "download_folder(MODEL_DIR, folder_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zszni5fThq_n"
      },
      "source": [
        "# Train Model (not recommended)\r\n",
        "Running this section will recreate the training data that was downloaded in the previous section and it will take considerable time to complete. I advise against running this section unless you really want to verify that absolutely everything works. Your results will also not necessarily be the same as mine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XO0Z1tOTKu_"
      },
      "source": [
        "This code block generate the \"Example List with Context\" files that will be used when training the model. Each the document list associated with each query is broken up into increments that are the size of 'list size'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B25volUOiwoi"
      },
      "source": [
        "!python $WORKDIR/bert_convert_json_to_elwc.py \\\r\n",
        "    --vocab_file $VOCAB_FILE \\\r\n",
        "    --sequence_length $SEQ_LENGTH \\\r\n",
        "    --query_file $JSON_DIR/bert_train_queries.json \\\r\n",
        "    --qrel_file $JSON_DIR/bert_train_qrels.json \\\r\n",
        "    --doc_file $JSON_DIR/bert_train_docs.json \\\r\n",
        "    --query_key $QUERY_TOKENS \\\r\n",
        "    --output_train_file \"$WORKDIR/tfrecord_data/train.bert.tfrecord\" \\\r\n",
        "    --output_eval_file \"$WORKDIR/tfrecord_data/eval.bert.tfrecord\" \\\r\n",
        "    --list_size $LIST_SIZE \\\r\n",
        "    --do_lower_case\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwYlcE9RSw48"
      },
      "source": [
        "The next code block deletes any existing tuning data for the model. If this was not cleared up then the model would start from the last point in the existing tuning data. Since the existing tuning data ends at a point greater than what this code uses, it will not run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asqbZBsSiiEe"
      },
      "source": [
        "!rm -rf $TUNED_MODEL_DIR"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BSdIG-xTv0X"
      },
      "source": [
        "The next script to be executed is what actually trains the model. This script was lifted with minimal changes from the tensorflow_ranking github repo described within the project documentation. The orginal version required the user to run bazel to compile the script and its dependcies into an executable and this version can be run directly via python. The script works by creating a pipeline that will train the model until it reachs the defined number of training steps. If the saved model located in 'TUNED_MODEL_DIR' already execeeds or equals this number to steps then it will note that and end without performing any training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBYlTF9pin9M"
      },
      "source": [
        "!python \"$WORKDIR/bert_train.py\" \\\r\n",
        "    --train_input_pattern=\"$WORKDIR/tfrecord_data/train.bert.tfrecord\" \\\r\n",
        "    --eval_input_pattern=\"$WORKDIR/tfrecord_data/eval.bert.tfrecord\" \\\r\n",
        "    --bert_config_file=$MODEL_DIR/bert_config.json \\\r\n",
        "    --bert_init_ckpt=$MODEL_DIR/bert_model.ckpt \\\r\n",
        "    --bert_max_seq_length=$SEQ_LENGTH \\\r\n",
        "    --model_dir=\"$TUNED_MODEL_DIR\" \\\r\n",
        "    --list_size=$LIST_SIZE \\\r\n",
        "    --loss=approx_ndcg_loss \\\r\n",
        "    --train_batch_size=1 \\\r\n",
        "    --eval_batch_size=1 \\\r\n",
        "    --learning_rate=1e-5 \\\r\n",
        "    --num_train_steps=20000 \\\r\n",
        "    --num_eval_steps=100 \\\r\n",
        "    --checkpoint_secs=100 \\\r\n",
        "    --num_checkpoints=5 \\\r\n",
        "    --config=cuda\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u6RfEcshrmy"
      },
      "source": [
        "# Predict with Model\r\n",
        "This section uses a trained model to predict the relevance scores of documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a083ANumXtR4"
      },
      "source": [
        "This is the most important block of code in the notebook. It runs the script that predicts the relevance score of a list of documents. The list of documents that will run predictions on are defined via the output of another ranker. There are two main reasons to run this model on the output of another ranker instead of ranking all documents in the corpus. First is that ranking all documents in the corpus can take a very long time. For this corpus it can take approximately 24 hrs or more. The second is that NCDG of the results generated when using the full corpus is very low but re-ranking the outputs of another rankers predictions generates more accurate results than the previous ranker and runs quickly.\r\n",
        "\r\n",
        "The script itself loads all the necessary documents and queries and then loops over the queries. The list of documents for each query is broken up into chunks of size 'docs_at_once' and then converted into ELWC style format. The query tokens that are used for each chunk are determined by the 'query_key' argument and for the model ranks will likely either be \"narrative_tokens\" or \"question_tokens\". The formatted chunks are then fed into the model to generate predictions. The list is broken up so that the GPU does not run out of memory while predicting.\r\n",
        "\r\n",
        "This script is a heavily modified version of a script the comes from the cognitiveai git repo that is recognized in the resources section of the project documentation. The intial version required compiling the script with bazel and using Docker to run a prediction server that would feed results back to the script via GRPC. This version of the script does neither of those things with no performance degredation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "queqXm9WC8vV"
      },
      "source": [
        "!mkdir $WORKDIR/scores\r\n",
        "!python $WORKDIR/bert_predict.py \\\r\n",
        "    --vocab_file $VOCAB_FILE \\\r\n",
        "    --sequence_length $SEQ_LENGTH \\\r\n",
        "    --query_file $JSON_DIR/bert_test_queries.json \\\r\n",
        "    --query_key $QUERY_TOKENS \\\r\n",
        "    --doc_file $JSON_DIR/bert_test_docs.json \\\r\n",
        "    --output_file $WORKDIR/scores/test_bert_scores.json \\\r\n",
        "    --model_path $TUNED_MODEL_DIR \\\r\n",
        "    --docs_at_once 500 \\\r\n",
        "    --rerank_file \"$BASE/predictions.txt\" \\\r\n",
        "    --do_lower_case\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJxtM6nrXrUu"
      },
      "source": [
        "This final code block outputs the predicionts file that was generated with the predictions script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT_xaJZKOcTv"
      },
      "source": [
        "!cat $BASE/predictions.txt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}